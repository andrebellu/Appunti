\documentclass[../main.tex]{subfiles}

\begin{document}

\section{Sistemi Lineari}
\subsection{Determinante}
Sia $A = (a_{ij})$ una matrice quadrata di ordine $n$, a elementi in un campo
$\mathbb{K}$. Si dice \textbf{determinante}, e si indica con $\det(A)$ o $|A|$,
la somma di tutti i suoi termini presi con il proprio segno. Cioè:
\[
    \det(A) = \sum_{\ah\in S_n}^{}sgn(\ah)a_{1\ah(1)}a_{2\ah(2)}\cdots a_{n\ah(n)}
\]

\subsubsection{Proprietà}
\begin{enumerate}
    \item Se una colonna (o una riga) di una matrice è nulla, allora il determinante è
          nullo.
    \item $\det(A) = \det({^{t}A})$, infatti, i termini estratti da ${^{t}A}$ sono tutti e soli i termini estratti da $A$. Sia $A= (a_{ij})$ e $B = ({^{t}A}) = (b_{ij})$ allora $b_{ij} = a_{ji}$. Se:
          \[
              b_{1\ah(1)}b_{2\ah(2)}\cdots b_{n\ah(n)}
          \]
          è un termine estratto da ${^{t}A}$, associato alla permutazione $\ah$, esso coincide con:
          \[
              a_{\ah(1)1}a_{\ah(2)2}\cdots a_{\ah(n)n}
          \]
          che per definizione, è un termine estratto da $A$ e individuato dalla
          permutazione $\ah^{-1}$. Dunquue, poichè $sgn (\ah) = sgn (\ah^{-1})$, possiamo
          concludere che $\det(A) = \det({^{t}A})$.
    \item Se $A'$ è ottenuta da $A$ scambiando tra loro due righe (o colonne), allora
          $\det(A') = -\det(A)$. Basta osservare che, i termini della matrice $A'$ si
          ottengono da quelli di $A$ scambiando tra loro due termini, e quindi, il segno
          del determinante cambia. Infatti, se $A'=(b_{ij})$ è ottenuta da $A= (a_{ij})$
          scambiando la k-esima riga con l'h-esima, allora ogni termine estratto da $A'$
          \[
              b_{1\ah(1)}\cdot\ldots\cdot b_{k\ah(k)}\cdot\ldots\cdot b_{h\ah(h)}\cdot\ldots\cdot b_{n\ah(n)}
          \]
          associato alla permutazione $\ah$, è uguale a:
          \[
              a_{1\ah(1)}\cdot\ldots\cdot a_{h\ah(k)}\cdot\ldots\cdot a_{k\ah(h)}\cdot\ldots\cdot a_{n\ah(n)}
          \]
          che risulta un termine di $A$ associato alla permutazione $(\sigma$ o $\ah)$,
          dove, $\sigma$ è lo scambio di $k$ con $h$. Ma essendo $sgn(\ah)=-sgn S (\ah\ o
              \ \sigma)$, risulta $|A'| = -|A|$
    \item Se $A$ ha due righe (o due colonne) uguali, allora $\det(A) = 0$. Infatti, se
          $A$ ha due righe uguali, allora, scambiando tra loro queste due righe, non si
          altera la matrice $A$, e per la precendente proprietà, $\det(A) = -\det(A)$, da
          cui $\det(A) = 0$.
    \item Se in $A$ una colonna $C_i$ è la somma di due n-uple $X_i, Y_i$, cioè se $A$ è
          del tipo:
          \[
              (C_1\ \cdots\ X_i + Y_i\ \cdots\ C_n)
          \]
          allora $|A|=|C_1 \cdots X_i \cdots C_n|+|C_1\cdots Y_i\cdots C_n|$.
          Analogamente per le righe.
    \item Se $A'$ è una matrice ottenuta da $A= (C_1 \ C_2 \cdots C_n)$ moltiplicando per
          $k\in\mathbb K$ una sua colonna (o riga), allora
          \[
              |A'|=|C_1\ \cdots\ kC_i\ \cdots\ C_n|=k|C_1\ \cdots\ C_i\ \cdots\ C_n|=k|A|
          \]
    \item Se $A$ ha due colonne (o due righe) proporzionali, allora $\det(A) = 0$.
    \item Se $A$ ha una colonna (o una riga) che è combinazione lineare di altre colonne
          (o righe), allora $\det(A) = 0$.
    \item Se $A'$ è una matrice ottenuta da $A$ sommando ad una sua colonna (o riga) un
          multiplo di un'altra colonna (o riga), allora $|A'| = |A|$.
\end{enumerate}

\subsection{Eliminazione di Gauss}
L'intendo del \textbf{metodo di eliminazione di Gauss} è quello di ridurre una
matrice $A$ ad una matrice $A'$, detta \textbf{ridotta a gradini}, in quanto il
determinante di quest'ultima può essere calcolato moltiplicando gli elementi
presenti nella diagonale principale, che ha la forma:
\[
    A' = \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        0      & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        0      & 0      & \cdots & a_{nn}
    \end{bmatrix}
\]
Per farlo utilizziamo quelle che si chiamano \textbf{Mosse di Gauss}:
\begin{enumerate}
    \item Scambiare tra loro due righe della matrice;
    \item Moltiplicare una riga per un numero diverso da zero;
    \item Sostituire ad una riga la somma di essa con un multiplo di un'altra riga.
\end{enumerate}
\textbf{Osservazione:} le mosse di Gauss non alterano il determinante della matrice.\\
Passi dell'algoritmo di Gauss:\\
Indichiamo con $A$ una matrice non ridotta a gradini con $m$ righe e $n$ colonne.
\begin{enumerate}
    \item Sia $C_k$, con $1\leq\ k\leq\ n$, la prima colonna a paritre da sinistra che
          contiene almeno un termine $a$ non nullo.\\ Detta $R_1$ la prima riga della
          matrice, possono presentarsi\textbf{ due eventualità}:
          \begin{enumerate}
              \item Se $a$ è un elemento di $R_1$, passiamo al punto $3$
              \item Se $a\notin\ R_1$. Controlliamo se la matrice ottenuta dopo lo scambio è
                    ridotta a gradini: se lo è possiamo fermarci, in caso contrario procediamo
                    oltre.
          \end{enumerate}
    \item L'obiettivo è annullare tutti gli elementi della k-esima colonna al di sotto di
          $a$. Sostituiamo ogni riga $R_i$, con $i>1$ e con k-esimo elemento non nullo,
          con $R_i+\lambda\ R_1, \lambda\in\mathbb{R}: \ R_i+\lambda\ R_1 = 0$.
    \item Se la matrice risultante è ridotta a gradini, allora l'algoritmo termina,
          altrimenti ripetiamo i passi precedenti con la matrice ottenuta.
\end{enumerate}

\subsection{Complemento algebrico}
Sia $A= (a_{ij})$ una matrice quadrata di ordine $n$, a elementi in un campo
$\mathbb{K}$. Si dice \textbf{complemento algebrico} dell'elemento $a_{hk}$, e
si indica con $\Gamma_{hk}$, il determinate della matrice quadrata di orfdine
$n-1$, ottenuta da $A$ cancellando la riga $h$ e la colonna $k$, preso con il
segno $ (-1)^{h+k} $.

\subsection{Teorema di Laplace I}
Data una matrice quadrata $A$ di ordine $n$, la somma dei prodotti degli
elementi di una sua riga (o colonna), per i rispettivi complementi algebrici, è
il determinate di $A$. Pertanto, la formula del calcolo del determinanto di $A=
    (a_{ij})$ rispetto alla i-esima riga è:
\[
    |A| = \sum_{j=1}^n a_{ij}\Gamma_{ij} \ \ \forall i = 1, 2, \ldots, n
\]
Rispetto alla j-esima colonna è:
\[
    |A| = \sum_{i=1}^n a_{ij}\Gamma_{ij} \ \ \forall j = 1, 2, \ldots, n
\]
L'utilizzo dei complementi algebrici consente, quindi il calcolo del
determinante di una matrice di ordine $n$ calcolando determinanti di matrici di
ordine inferiore.

\subsection{Teorema di Laplace II}
Sia $A$ una matrice quadrata di ordine $n$. La somma dei prodotti degli
elementi di una sua riga (o colonna) per i complementi elgebrici degli elementi
di un'altra riga (o colonna) vale zero.

\subsection{Teorema di Binet}
Date due matrici quadrate di ordine $n$, $A$ e $B$, il determinante della
matrice prodotto $AB$ è uguale al prodotto dei determinanti delle due matrici:
\[
    |AB| = |A||B|
\]

\subsection{Matrici Invertibili}
Una matrice quadrata $A$ di ordine $n$ si dice \textbf{invertibile} se esiste
una matrice $B$ quadrata e dello stesso ordine, tale che $AB = BA = I_n$, dove
$I_n$ è la matrice identità di ordine $n$. In tal caso, la matrice $B$ si dice
\textbf{matrice inversa} di $A$ e si indica con $A^{-1}$.

\subsubsection{Teorema}
Una matrice quadrata $A = (a-{ij})$, di ordine $n$, è invertibile
$\iff|A|\ne0$. In questo caso, la matrice inversa di $A$ risulta essere
$A^{-1}=|A|^{-1}\ {^{t}A_a}$ dove ${^{t}A_a}$ è la trasposta dell'aggiunta di
$A$.

\subsection{Dipendenza lineare e determinanti}
Data una matrice $A\in{\mathbb{K}^{m,n}} (K)$ si dice \textbf{minore di ordine
    k}, estratto da $A$, una matrice quadrata di ordine $k$ (ovviaente $k\leq{m}$ e
$k\leq{n}$) ottenuta da $A$ cancellando $m-k$ righe e $n-k$ colonne.\\

\subsubsection{Teorema}
Una sequenza $S = (\s{v}{k})$ di $k$ vettori $ (k\leq{n})$ dello spazio
vettoriale $\Vx{n}$ è libera $\iff$ dalla matrice $A$, che ha nelle proprie
righe (o colonne) le componenti dei vettori di $S$ in una base $B$ di $\Vx{n}$,
si può estrarre un minore di ordine $k$ con determinante non nullo.

\subsection{Rango}
Sia $A$ una matrice di $K^{m,n} (\mathbb{K})$. Si dice \textbf{rango} della
matrice $A$, e si indica con $rK (A)$, il massimo ordine di un minore non nullo
estratto da $A$.\\ In modo equivalente, il rango di una matrice $A$ è $p$
quando esiste un minore di ordine $p$ non nullo, ma non esiste alcun minore di
ordine $p+1$ non nullo.\\

\subsubsection{Osservazioni}
Data una matrice $A \in K^{m,n} (\mathbb{K})$
\begin{enumerate}
    \item $rK (A) = 0\iff{} A$ è la matrice nulla;
    \item il rango di $A$ coincide con il rango della sua trasposta ${^{t}A}$;
    \item $rK (A) \leq min\{m,n\}$;
    \item se $B$ è una matrice di $K^{n,p} (\mathbb{K})$, il rango della matrice prodotto
          $AB$ è minore o guale, si adel rango di $A$, che di queòòp di $B$.
    \item se $A$ e $B$ sono matrici quadrate dello stesso ordine e $A$ è invertibile,
          allora $rK (AB) = rK (BA) = rK (B)$.
\end{enumerate}

\subsection{Kronecker}
Gli spazi vettoriali $\Span(R)$ ed $\Span(C)$, di una matrice $A\in K^{m,n}
    (\mathbb{K})$, hanno la stessa dimensione e tale dimensione coincide con il
rango di $A$.

\subsection{Osservazione}
Il rango di una matrice $A$ coincide con il massimo numero di righe o di
colonne linearmente indipendenti estraibili dalla matrice $A$.

\subsubsection{Corollario}
Se $A$ è una matrice quadrata di ordine $n$, con elementi in un campo
$\mathbb{K}$, le seguenti condizioni sono equivalenti:
\begin{enumerate}
    \item $|A|\ne0$;
    \item $A$ è invertibile
    \item $rK (A) = n$;
    \item le righe sono linearmente indipendenti e, quindi sono base di $\mathbb{K}^n$;
    \item le colonne sono linearmente indipendenti e, quindi sono base di $\mathbb{K}^n$;
\end{enumerate}

\subsection{Teorema degli orlati}
Una matrice $A\in K^{m,n} (\mathbb{K})$ ha rango $r$ se e solo se esiste un
minore $M$ di ordine $r$ a determinante non nullo e tutti i minori di ordine
$r+1$, che contengono $M$, hanno determinante nullo.

\subsection{Sistemi Lineari}
Un \textbf{sistema lineare} è insieme di $m$ equazioni lineari in $n$ incognite
a coefficineti in un campo $\mathbb{K}$, un sistema lineare si può
rappresentare come:
\[
    \begin{cases}
        a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = b_1 \\
        a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = b_2 \\
        \vdots                                           \\
        a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n = b_m
    \end{cases}
\]
con $a_{ij}, b_i \in \mathbb{K}$. Gli elementi $a_{ij}$ si chiamano
coefficienti delle incognite, gli elementi $b_i$ si chiamano termini noti.\\ La
matrice $m\times n$
\[
    A = \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{bmatrix}
\]
è detta matrice dei coefficienti o \textbf{matrice incompleta} del sistema.\\
La matrice $n\times 1$
\[
    X = \begin{bmatrix}
        x_1    \\
        x_2    \\
        \vdots \\
        x_n
    \end{bmatrix}
\]
è detta matrice delle incognite.\\
La matrice $m\times 1$
\[
    B = \begin{bmatrix}
        b_1    \\
        b_2    \\
        \vdots \\
        b_m
    \end{bmatrix}
\]
è detta matrice dei termini noti.\\
Infine, la matrice $m\times (n+1)$
\[
    A|B = \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} & b_1    \\
        a_{21} & a_{22} & \cdots & a_{2n} & b_2    \\
        \vdots & \vdots & \ddots & \vdots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn} & b_m
    \end{bmatrix}
\]
è detta \textbf{matrice completa} del sistema.

\subsubsection{Sistema omogeneo}
Un sistema lineare si dice \textbf{omogeneo} se tutti i termini noti sono
nulli. Utilizzando il prodotto tra matrici, il sistema lineare assume la
seguente forma:
\[
    AX = B
\]
In particolare, un sistema lineare omogeneo, in forma matriciale, si scrive
\[
    AX = \underbar{0}
\]
Si è soliti chiamare \textbf{sistema lineare omogeneo associato} ad $AX = B$,
il sistema lineare omogeneo ottenuto da $AX = B$ ponendo $B = \underbar{0}$.

\subsubsection{Sistema compatibile}
Sia $AX = B$ un sistema lineare in $m$ equazioni e $n$ incognite. Si dice che
tale sistema ha soluzione, ovvero che il \textbf{sistema è compatibile}, se
esiste almeno un $n$-upla $ (\s{\ah}{n})$ di elementi di $\mathbb{K}$ che
risolve tutte le equazioni del sistema. Tale $n$-upla è detta
\textbf{soluzione}.\\ \textbf{Osservazione:} affermare hce una $n$-upla $
    (\s{\ah}{n})$ di elementi di $\mathbb{K}$ è soluzione di un sistema $AX = B$,
pensando tale sistema scritto nella forma, equivale a dire che
\[
    \ah_1C_1 + \ah_2C_2 + \cdots + \ah_nC_n = B
\]
cioè che $B$ è combinazione lineare delle colonne della matrice $A$ secondo i
coefficienti $\ah_1, \ah_2, \ldots, \ah_n$.

\subsection{Rouché-Capelli}
Un sistema lineare $AX = B$, in $m$ equazioni e $n$ incognite, è compatibile
$\iff rk (A) = rk (A|B)$.

\subsubsection{Dimostrazione}
Se il sistema $AX = B$ ha soluzione, esiste una $n$-upla di $\mathbb{K}$ $
    (\s{\ah}{n})$, che ne soddisfa tutte le equazioni, tale, cioè, che:
\[
    \ah_1C_1 + \ah_2C_2 + \cdots + \ah_nC_n = B
\]
e quindi, B risulta essere combinazione lineare delle colonne della matrice A.
Pertanto, il massimo numero di colonne linearmente indipendenti, estraibili
dalla matrice $A$, coincide con il massimo numero di colonne linearmente
indipendeti, estraibili dalla matrice $A|B$, e $rK (A) = rK (A|B)$. Viceversa
se $rK (A) = rK (A|B)$, allora il massimo numero di colonne linearmente
indipendenti, estraibili dalla matrice $A$, coincide con il massimo numero di
colonne linearmente indipendenti, estraibili dalla matrice $A|B$, di
conseguenza, la colonna $B$ risulta una combinazione lineare delle colonne
della matrice $A$, e quindi, esiste una $n$-upla di $\mathbb{K}$ $
    (\s{\ah}{n})$, tale che:
\[
    \ah_1C_1+\ah_2C_2+\cdots+\ah_nC_n = B
\]
Pertanto, il sistema $AX = B$ ha soluzione.

\subsection{Cramer}
Un sistema lineare $AX=B$, in $n$ equazioni ed $n$ incognite, in cui $|A|\ne0$,
ammette una e suna sola soluzione.

\subsubsection{Dimostrazione}
Proviamo che, se la soluzione esiste, allora è unica. Supponiamo che $X_1$ e $
    X_2$ siano due soluzioni del sistema $AX = B$, che sia cioè:
\[
    AX_1 = B \ \ \text{ e } \ \ AX_2 = B
\]
e quindi, $AX_1=AX_2$. Essendo $|A|\ne0$, la matrice $A$ è invertibile,
pertando, moltiplicando la precendente uguaglianza a sinistra per $A^{-1}$, si
ottiene appunto che $X_1 = X_2$ e dunque la soluzione è unica. Dimostriamo ora
che la soluzione esiste. Indichiamo con $B_i$ la matrice ottenuta da $A =
    (\s{C}{n})$ sostituendo la colonna $i$-esima con la colonna $B$ dei termini
noti. Sia, cioè, $B_i = (C_{1}\cdots C_{i-1} \ B \ C_{i+1}\cdots C_n)$.
Verifichiamo che:
\[
    {^{t}\bar{X}} = \left(\dfrac{|B_1|}{|A|}\ \dfrac{|B_2|}{|A|}\ \cdots\ \dfrac{|B_n|}{|A|}\right)
\]
è soluzione del sistema $AX = B$. Essendo, infatti, la matrice $A$ quadrata e invertibile, da $AX = B$, moltiplicando ambo i membri a sinistra per $A^{-1}$, è possibile osservare che $A^{-1}B$ risolve il sistema $AX = B$. Si osserva che $A^{-1}B$ è proprio la soluzione di ${^{t}\bar{X}}$.

\subsection{Sistema principale equivalente}
Sia $AX = B$ un sistema lineare in $m$ equazioni e $n$ incognite, cioè sia $rk
    (A) = rK (A|B) = p$. Si dice \textbf{sistema principale equivalente} ad $AX =
    B$, un sistema $A'X = B'$ ottenuto estraendo $p$ equazioni del sistema $AX =
    B$, in modo tale che $rk (A') = rk (A'|B') = p$.

\subsubsection{Teorema}
Ogni sistema compatibile ha le stesse soluzioni di uno suo qualunque sistema
principale equivalente.

\subsubsection{Teorema}
Sia $AX = \underbar{0}$ un sistema lineare omogeneo in $m$ equazioni e $n$
incognite. L'insieme $S$ delle sue soluzioni è un sottospazio di $\mathbb{K}^n
    ()\mathbb{K}$ di dimensione $n-rK (A)$.

\end{document}